{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4addf164",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse.linalg import eigsh, ArpackNoConvergence\n",
    "from scipy.optimize import curve_fit, OptimizeWarning\n",
    "from scipy.sparse import csr_matrix, csc_matrix\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.datasets import make_swiss_roll\n",
    "from scipy.sparse.csgraph import laplacian\n",
    "from scipy.fftpack import fft2, ifft2\n",
    "from collections import namedtuple\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "import multiprocessing\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import warnings\n",
    "import zipfile\n",
    "import imageio\n",
    "import pickle\n",
    "import tqdm\n",
    "import os\n",
    "import io\n",
    "from params import p\n",
    "project_name = \"sample\"\n",
    "eps = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9351f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the project name to before loading the parameters\n",
    "p.proj_name = project_name\n",
    "# When we call p.load(), it should attempt to load \"params_sample.toml\"\n",
    "p.load()\n",
    "# Print the parameters to verify they are loaded correctly\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a70f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NullEmitter:\n",
    "    \"\"\"\n",
    "    A class that provides a no-operation (no-op) implementation of an emitter.\n",
    "\n",
    "    This class is designed to be used in contexts where an emitter is required by the interface,\n",
    "    but no actual emitting action is desired. It effectively serves as a placeholder or a stub\n",
    "    that satisfies the requirement of having an emitter without performing any operation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def emit(self, percent):\n",
    "        \"\"\"\n",
    "        A no-operation implementation of the emit method.\n",
    "\n",
    "        This method is intended to fulfill the interface requirement for an emitting action\n",
    "        without performing any actual work. It can be used to ignore progress updates or\n",
    "        other emitting actions in a safe and controlled manner.\n",
    "\n",
    "        Parameters:\n",
    "        - percent (any): This parameter is accepted to match the expected interface of an\n",
    "          emitter method, but it is not used within the method. Any value passed to this\n",
    "          parameter will be ignored.\n",
    "\n",
    "        Returns:\n",
    "        None\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5717b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_print(msg: str=\"\"):\n",
    "    \"\"\"\n",
    "    Prints a debug message along with the caller's stack trace.\n",
    "\n",
    "    Parameters:\n",
    "    - msg (str): The debug message to print. If empty, only the stack trace is printed.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    if msg:\n",
    "        print(msg)\n",
    "    stack = traceback.format_stack()\n",
    "    print(stack[-2].split('\\n')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe04c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fin1(filename):\n",
    "    \"\"\"\n",
    "    Reads and returns the data from a pickle file.\n",
    "\n",
    "    This function attempts to open a file in binary read mode and deserialize its contents using\n",
    "    pickle. If an exception occurs during this process, it logs the exception message using\n",
    "    `debug_print` and returns None.\n",
    "\n",
    "    Parameters:\n",
    "    - filename (str): The path to the file to be read.\n",
    "\n",
    "    Returns:\n",
    "    - The deserialized data from the file if successful, None otherwise.\n",
    "    \"\"\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        try:\n",
    "            data = pickle.load(f)\n",
    "            return data\n",
    "        except Exception as e:\n",
    "            debug_print(str(e))\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f17b792",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fout1(filename, **kwargs):\n",
    "    \"\"\"\n",
    "    Writes the given keyword arguments to a file using pickle serialization.\n",
    "\n",
    "    This function opens a file in binary write mode and serializes the keyword arguments passed to\n",
    "    it using pickle, with the highest available protocol. It ensures that the data is written\n",
    "    efficiently and securely.\n",
    "\n",
    "    Parameters:\n",
    "    - filename (str): The path to the file where the data will be written.\n",
    "    - **kwargs: Arbitrary keyword arguments that will be serialized and written to the file.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(kwargs, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    # Note: The file is automatically closed when exiting the 'with' block, so f.close() is not necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3d7306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9570bc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def L2_distance(a, b):\n",
    "    \"\"\"\n",
    "    Computes the Euclidean distance matrix between two sets of vectors.\n",
    "\n",
    "    This function calculates the pairwise Euclidean (L2) distances between vectors in two sets,\n",
    "    represented by matrices A and B. The computation is vectorized for efficiency, making use\n",
    "    of broadcasting and matrix operations to avoid explicit loops over elements.\n",
    "\n",
    "    Parameters:\n",
    "        a (np.ndarray): A D x M array where D is the dimensionality of each vector and M is the number\n",
    "                        of vectors in the first set.\n",
    "        b (np.ndarray): A D x N array where D is the dimensionality of each vector (matching the first set)\n",
    "                        and N is the number of vectors in the second set.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: An M x N array containing the Euclidean distances between each pair of vectors\n",
    "                    from the first set to the second set.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the input matrices A and B do not have the same dimensionality (i.e., the number\n",
    "                    of rows D does not match).\n",
    "\n",
    "    Note:\n",
    "        - The function ensures numerical stabil ity by setting very small negative values (which can\n",
    "          arise due to floating point arithmetic errors) to zero before taking the square root.\n",
    "        - This implementation assumes that both input matrices are real-valued.\n",
    "    \"\"\"\n",
    "    eps = 1e-8  # Small epsilon value for numerical stability\n",
    "\n",
    "    # Check if input matrices have the same dimensionality\n",
    "    if a.shape[0] != b.shape[0]:\n",
    "        raise ValueError(\"A and B should be of same dimensionality\")\n",
    "    \n",
    "    # Compute squared magnitudes of each vector in A and B\n",
    "    aa = np.sum(a**2, axis=0)\n",
    "    bb = np.sum(b**2, axis=0)\n",
    "    \n",
    "    # Compute the matrix product of A transposed and B\n",
    "    ab = np.matmul(a.T, b)\n",
    "    \n",
    "    # Use the identity ||a - b||^2 = ||a||^2 + ||b||^2 - 2 * a^T * b to compute distance matrix\n",
    "    tmp = aa[:, np.newaxis] + bb[np.newaxis, :] - 2 * ab\n",
    "    \n",
    "    # Ensure no negative values due to floating point errors\n",
    "    tmp[tmp < eps] = 0\n",
    "    \n",
    "    # Return the square root of the distance matrix to get Euclidean distances\n",
    "    return np.sqrt(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735d31b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svdRF(A):\n",
    "    \"\"\"\n",
    "    Performs a singular value decomposition-like operation on matrix A using eigenvalue decomposition.\n",
    "    \n",
    "    This function is tailored for matrices that are not necessarily square, calculating the eigendecomposition\n",
    "    of A^T*A or A*A^T as appropriate based on the shape of A. It then tidies up the eigenvalues and eigenvectors\n",
    "    to align with the conventional output of SVD, providing matrices U, S, and V such that A â‰ˆ U*S*V^T.\n",
    "\n",
    "    Parameters:\n",
    "        A (np.ndarray): A D1 x D2 matrix for which the SVD-like decomposition is to be performed.\n",
    "                        A can be of any shape, not necessarily square.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing three elements:\n",
    "               - U (np.ndarray): An orthogonal matrix containing the left singular vectors of A.\n",
    "               - S (np.ndarray): A diagonal matrix with the square roots of the eigenvalues of A^T*A or A*A^T\n",
    "                                 along the diagonal, representing the singular values of A.\n",
    "               - V (np.ndarray): An orthogonal matrix containing the right singular vectors of A.\n",
    "\n",
    "    Details:\n",
    "        The function internally defines a `tidyUp` helper function to sort the eigenvalues and eigenvectors,\n",
    "        and to calculate the matrices S (singular values) and invS (inverse of S). The choice of decomposing\n",
    "        A^T*A or A*A^T is based on the dimensions of A to ensure computational efficiency.\n",
    "\n",
    "    Note:\n",
    "        This function does not directly use the SVD function from numpy.linalg but achieves a similar result\n",
    "        through eigenvalue decomposition, which can be more efficient or numerically stable in certain contexts.\n",
    "    \"\"\"\n",
    "    def tidyUp(D, EV):\n",
    "        # Sort the eigenvalues and eigenvectors in descending order\n",
    "        order = np.argsort(D)[::-1]\n",
    "        D = np.sort(D)[::-1]\n",
    "        EV = EV[:, order]\n",
    "        \n",
    "        # Calculate the matrices for singular values and their inverses\n",
    "        sqrtD = np.sqrt(D)\n",
    "        S = np.diag(sqrtD)\n",
    "        invS = np.diag(1. / sqrtD)\n",
    "        \n",
    "        return (D, EV, S, invS)\n",
    "\n",
    "    # Determine the shape of A and perform eigendecomposition appropriately\n",
    "    D1, D2 = A.shape\n",
    "    if D1 > D2:\n",
    "        # For tall matrices, decompose A^T*A\n",
    "        D, V = np.linalg.eigh(np.matmul(A.T, A))\n",
    "        D, V, S, invS = tidyUp(D, V)\n",
    "        U = np.matmul(A, np.matmul(V, invS))\n",
    "    else:\n",
    "        # For wide matrices, decompose A*A^T\n",
    "        D, U = np.linalg.eigh(np.matmul(A, A.T))\n",
    "        D, U, S, invS = tidyUp(D, U)\n",
    "        V = np.matmul(A.T, np.matmul(U, invS))\n",
    "    \n",
    "    return (U, S, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63a7874",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wiener(CTF, posPath, posPsi1, ConOrder, num):\n",
    "    \"\"\"\n",
    "    Computes the Wiener filter domain for a set of CTF values.\n",
    "\n",
    "    This function calculates the Wiener filter domain based on the given CTF values, taking into account\n",
    "    the position path, the position in the Psi coordinate, the continuity order, and the total number of\n",
    "    elements. It is particularly useful in cryo-EM image processing for deconvolving images with known\n",
    "    CTF distortions under a specified signal-to-noise ratio (SNR).\n",
    "\n",
    "    Parameters:\n",
    "        CTF (np.ndarray): A 3D array of CTF values with shape (N, dim, dim), where N is the number of CTF\n",
    "                          patterns, and 'dim' is the dimensionality of each CTF pattern.\n",
    "        posPath (np.ndarray): An array of indices specifying the order in which CTF patterns are considered\n",
    "                              in the analysis.\n",
    "        posPsi1 (int): The position index within the Psi coordinate, indicating the specific CTF pattern\n",
    "                       to start with in the computation.\n",
    "        ConOrder (int): The continuity order, specifying how many adjacent CTF patterns to consider for\n",
    "                        averaging in the Wiener domain calculation.\n",
    "        num (int): The total number of elements or CTF patterns to include in the computation from the\n",
    "                   starting position posPsi1.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing two elements:\n",
    "               - wiener_dom (np.ndarray): The computed Wiener filter domain, a 3D array with shape\n",
    "                                         (num - ConOrder, dim, dim), representing the filtered domain for\n",
    "                                         each considered CTF pattern.\n",
    "               - CTF1 (np.ndarray): The subset of CTF patterns used in the Wiener domain calculation,\n",
    "                                    with shape (num - ConOrder, dim, dim).\n",
    "\n",
    "    Notes:\n",
    "        - The function assumes a constant signal-to-noise ratio (SNR) for the simplification of the Wiener\n",
    "          filter calculation. The SNR is hardcoded as 5, but this value can be adjusted based on specific\n",
    "          requirements or experimental data.\n",
    "        - The Wiener filter domain is calculated by summing the squares of the selected CTF patterns and\n",
    "          then adjusting for the SNR, following the principles of Wiener deconvolution in the frequency domain.\n",
    "    \"\"\"\n",
    "    dim = CTF.shape[1]  # Extract the dimensionality of each CTF pattern\n",
    "    SNR = 5  # Signal-to-Noise Ratio, a predefined constant for the calculation\n",
    "\n",
    "    # Select the specific subset of CTF patterns based on posPath and posPsi1\n",
    "    CTF1 = CTF[posPath[posPsi1], :, :]\n",
    "\n",
    "    # Initialize the Wiener domain array\n",
    "    wiener_dom = np.zeros((num - ConOrder, dim, dim), dtype='float64')\n",
    "\n",
    "    # Compute the Wiener domain by summing the squared CTF patterns and adjusting for SNR\n",
    "    for i in range(num - ConOrder):\n",
    "        for ii in range(ConOrder):\n",
    "            ind_CTF = ConOrder - ii + i\n",
    "            wiener_dom[i, :, :] += CTF1[ind_CTF, :, :]**2\n",
    "    wiener_dom += 1. / SNR  # Adjust for SNR\n",
    "\n",
    "    return (wiener_dom, CTF1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08a4cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class _Params:\n",
    "    \"\"\"\n",
    "    A class to hold configuration parameters for an iterative fitting or optimization process.\n",
    "\n",
    "    This class encapsulates various parameters that control the behavior of an iterative process, such as fitting a model to data or optimizing a function. It is designed to be flexible, allowing for the easy adjustment of key parameters affecting the convergence and accuracy of the process.\n",
    "\n",
    "    Attributes:\n",
    "        nDim (int): The dimensionality of the data or parameter space. Defaults to 3.\n",
    "        maxIter (int): The maximum number of iterations to perform in the optimization or fitting process. Defaults to 100.\n",
    "        delta_a_max (float): The maximum allowed change in parameter 'a' between iterations, controlling convergence criteria. Defaults to 1.0.\n",
    "        delta_b_max (float): The maximum allowed change in parameter 'b' between iterations, controlling convergence criteria. Defaults to 1.0.\n",
    "        delta_tau_max (float): The maximum allowed change in parameter 'tau' between iterations, controlling convergence criteria. Defaults to 0.01.\n",
    "        a_b_tau_result (str): The filename or path to save the result of the optimization or fitting process, specifically the optimized parameters 'a', 'b', and 'tau'. Defaults to 'a_b_tau_result.pkl'.\n",
    "        p (Any): A placeholder for an additional parameter, purpose and type to be determined based on specific use case. Initially None.\n",
    "        x (Any): A placeholder for the dataset or input data to which the optimization or fitting process is applied. Initially None.\n",
    "        x_fit (Any): A placeholder for the fitted data or output of the optimization process. Initially None.\n",
    "\n",
    "    Note:\n",
    "        - The class is designed with flexibility in mind, allowing for the addition or modification of parameters as needed for specific applications.\n",
    "        - The exact nature and use of the `p`, `x`, and `x_fit` attributes are intentionally left undefined to accommodate a wide range of possible use cases.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.nDim: int = 3\n",
    "        self.maxIter: int = 100\n",
    "        self.delta_a_max: float = 1.\n",
    "        self.delta_b_max: float = 1.\n",
    "        self.delta_tau_max: float = 0.01\n",
    "        self.a_b_tau_result: str = 'a_b_tau_result.pkl'\n",
    "        self.p = None\n",
    "        self.x = None\n",
    "        self.x_fit = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5787b06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _R_p(tau_p, a):\n",
    "    \"\"\"\n",
    "    Calculates the residual value(s) for a given tau parameter and a set of model parameters.\n",
    "\n",
    "    This function computes the sum of squared errors (SSE) between the model predictions and actual data,\n",
    "    given a specific value of the tau parameter. The model appears to involve a trigonometric relationship\n",
    "    between input data and parameters, highlighting its potential use in periodic or oscillatory systems.\n",
    "\n",
    "    Parameters:\n",
    "        tau_p (float or np.ndarray): The tau parameter value(s) at which the residuals are to be calculated.\n",
    "                                     This can be a single value or an array of values.\n",
    "        a (_Params): An instance of the _Params class containing the configuration and parameters for the\n",
    "                     model fitting process. It must have the following attributes defined:\n",
    "                     - nDim: The dimensionality of the data or parameter space.\n",
    "                     - a: The scaling factor(s) applied to the cosine component of the model.\n",
    "                     - b: The offset or bias term in the model.\n",
    "                     - x: The dataset or input data to which the model is being fitted.\n",
    "                     - p: The index or indices of the data points being considered in the calculation.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: An array of residual value(s) corresponding to the input tau parameter value(s).\n",
    "                    Each element in the array represents the SSE for a given value of tau_p.\n",
    "\n",
    "    Note:\n",
    "        - The function assumes that the input data `x`, model parameters `a` and `b`, and the index `p`\n",
    "          are all attributes of the `a` (_Params) instance. This design implies that the function is\n",
    "          closely tied to the specific structure and use case of the _Params class.\n",
    "        - The calculation involves a cosine function, suggesting that the model may be used to fit\n",
    "          data exhibiting periodic or oscillatory behavior.\n",
    "    \"\"\"\n",
    "    # Create an array of integers from 1 to nDim inclusive, representing dimensions or components\n",
    "    jj = np.array(range(1, a.nDim + 1))\n",
    "    \n",
    "    # Calculate the cosine argument based on tau_p and dimension indices\n",
    "    j_pi_tau_p = tau_p * jj * np.pi\n",
    "    \n",
    "    # Apply the model (a cosine function scaled by 'a.a' and offset by 'a.b') to calculate predictions\n",
    "    a_cos_j_pi_tau_p = a.a * np.cos(j_pi_tau_p)\n",
    "    \n",
    "    # Compute the error between model predictions and actual data for the specified data point(s)\n",
    "    err = a.x[a.p, :] - a.b - a_cos_j_pi_tau_p\n",
    "    \n",
    "    # Calculate and return the sum of squared errors\n",
    "    fval = np.sum(err**2, axis=1)\n",
    "    return fval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553e2233",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _solve_d_R_d_tau_p_3D(a: _Params):\n",
    "    '''\n",
    "    Solves for the tau value that minimizes the residual in a 3D model fitting process.\n",
    "\n",
    "    This function calculates the tau parameter for a data point 'p' that minimizes the residual\n",
    "    of a model defined by the equation x_ij = a_j * cos(j*pi*tau_i) + b_j, where 'j' ranges from 1 to 3,\n",
    "    making it specifically designed for 3D systems.\n",
    "\n",
    "    The process involves solving the derivative of the residual with respect to tau, setting it to zero,\n",
    "    and finding the roots of the resulting polynomial equation.\n",
    "\n",
    "    Parameters:\n",
    "        a (_Params): An instance of the _Params class containing all the necessary parameters\n",
    "                     and configurations for the model fitting process. It includes:\n",
    "                     - a: Coefficients for the model as a 1 x 3 array.\n",
    "                     - b: Offset terms for the model as a 1 x 3 array.\n",
    "                     - x: Data points to be fitted as an nS x 3 array.\n",
    "                     - p: Index of the data point of interest within 'x'.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "               - tau (float): The value of tau for the data point 'p' that minimizes the residual.\n",
    "               - beta (np.ndarray): The roots found from the derivative of the residual with respect\n",
    "                                    to tau, including both physical and non-physical solutions.\n",
    "\n",
    "    Note:\n",
    "        The function identifies candidate tau values by solving for the roots of the derivative of\n",
    "        the residual and applies physical constraints to select valid solutions. Non-real or out-of-bound\n",
    "        roots are discarded, ensuring the chosen tau is physically meaningful.\n",
    "    '''\n",
    "    # Construct polynomial coefficients for the derivative of the residual\n",
    "    d_R_d_beta_3D = np.array([\n",
    "        48 * a.a[2]**2, 0, 8 * a.a[1]**2 - 48 * a.a[2]**2, -12 * a.a[2] * (a.x[a.p, 2] - a.b[2]),\n",
    "        a.a[0]**2 - 4 * a.a[1]**2 + 9 * a.a[2]**2 - 4 * a.a[1] * (a.x[a.p, 1] - a.b[1]),\n",
    "        -a.a[0] * (a.x[a.p, 0] - a.b[0]) + 3 * a.a[2] * (a.x[a.p, 2] - a.b[2])\n",
    "    ]).T\n",
    "\n",
    "    # Solve the polynomial equation to find roots\n",
    "    beta = np.roots(d_R_d_beta_3D)\n",
    "\n",
    "    # Filter out roots with non-zero imaginary parts\n",
    "    tmp = np.absolute(np.imag(beta)) > 0\n",
    "    tmp = np.nonzero(tmp)[0]\n",
    "    beta1 = np.delete(beta, tmp, None)\n",
    "\n",
    "    # Further filter roots with absolute values greater than 1\n",
    "    tmp = np.absolute(beta1) > 1\n",
    "    tmp = np.nonzero(tmp)[0]\n",
    "    beta = np.real(np.delete(beta1, tmp, None))\n",
    "\n",
    "    # Convert valid roots to candidate tau values\n",
    "    tau_candidate = np.vstack((np.arccos(beta.reshape(-1, 1)) / np.pi, 0, 1))\n",
    "\n",
    "    # Select the tau value that minimizes the residual\n",
    "    id = np.argmin(_R_p(tau_candidate, a))\n",
    "    tau = tau_candidate[id]\n",
    "\n",
    "    return tau, beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46865434",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_fit_params(psi):\n",
    "    \"\"\"\n",
    "    Computes initial fitting parameters for a given input matrix 'psi', involving complex\n",
    "    mathematical operations to estimate parameters for a 3D model fitting process.\n",
    "\n",
    "    This function calculates initial guesses for parameters {a, b, tau} based on the input 'psi'.\n",
    "    It utilizes polynomial fitting to estimate these parameters, specifically focusing on \n",
    "    the fitting of data in 2D for {a, b} and setting derivatives to zero for {tau}. \n",
    "    The process includes solving a series of linear equations and polynomial expressions to \n",
    "    find these initial parameter guesses.\n",
    "\n",
    "    Parameters:\n",
    "        psi (np.ndarray): A 2D numpy array containing the data points for which the initial\n",
    "                          fitting parameters are to be calculated.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing two elements:\n",
    "               - np.ndarray: An array of tau values, representing one of the fitting parameters.\n",
    "               - _Params: An instance of the _Params class populated with the initial fitting parameters,\n",
    "                          including maximum iterations, delta values, result paths, and the calculated\n",
    "                          values of a, b, and tau.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If certain calculations result in invalid values that cannot be used for further\n",
    "                    computation, such as a negative discriminant during square root operations.\n",
    "\n",
    "    Note:\n",
    "        This function assumes 'psi' is structured with specific columns representing different \n",
    "        dimensions of the data. It is part of a larger fitting process, where these initial parameters\n",
    "        are refined through iterative optimization methods.\n",
    "    \"\"\"\n",
    "    a = _Params()\n",
    "    a.maxIter = 100  # maximum number of iterations, each iteration determines optimum sets of {a,b} and {\\tau} in turns\n",
    "    a.delta_a_max = 1  # maximum percentage change in amplitudes\n",
    "    a.delta_b_max = 1  # maximum percentage change in offsets\n",
    "    a.delta_tau_max = 0.01  # maximum change in values of tau\n",
    "    a.a_b_tau_result = 'a_b_tau_result.pkl'  # save final results here\n",
    "    nS = psi.shape[0]\n",
    "    a.x = psi[:, 0:3]\n",
    "    # initial guesses for {a,b} obtained by fitting data in 2D\n",
    "    # initial guesses for {tau} obtained by setting d(R)/d(tau) to zero\n",
    "    X = psi[:, 0]\n",
    "    Z = psi[:, 2]\n",
    "    X2 = X * X\n",
    "    X3 = X2 * X\n",
    "    X4 = X2 * X2\n",
    "    X5 = X3 * X2\n",
    "    X6 = X3 * X3\n",
    "    sumX = np.sum(X)\n",
    "    sumX2 = np.sum(X2)\n",
    "    sumX3 = np.sum(X3)\n",
    "    sumX4 = np.sum(X4)\n",
    "    sumX5 = np.sum(X5)\n",
    "    sumX6 = np.sum(X6)\n",
    "    sumZ = np.sum(Z)\n",
    "    sumXZ = np.dot(X.T, Z)\n",
    "    sumX2Z = np.dot(X2.T, Z)\n",
    "    sumX3Z = np.dot(X3.T, Z)\n",
    "    A = np.array([[sumX6, sumX5, sumX4, sumX3], [sumX5, sumX4, sumX3, sumX2], [sumX4, sumX3, sumX2, sumX],\n",
    "                  [sumX3, sumX2, sumX, nS]])\n",
    "    b = np.array([sumX3Z, sumX2Z, sumXZ, sumZ])\n",
    "    coeff = np.linalg.lstsq(A, b, rcond=None)[0]\n",
    "    D = coeff[0]\n",
    "    E = coeff[1]\n",
    "    F = coeff[2]\n",
    "    G = coeff[3]\n",
    "    disc = E * E - 3 * D * F\n",
    "    if disc < 0:\n",
    "        disc = 0.\n",
    "    if np.absolute(D) < 1e-8:\n",
    "        D = 1e-8\n",
    "    a1 = (2. * np.sqrt(disc)) / (3. * D)\n",
    "    a3 = (2. * disc**(3 / 2.)) / (27. * D * D)\n",
    "    b1 = -E / (3 * D)\n",
    "    b3 = (2. * E * E * E) / (27. * D * D) - (E * F) / (3 * D) + G\n",
    "    Xb = X - 2 * b1\n",
    "    Y = psi[:, 1]\n",
    "    XXb = X * Xb\n",
    "    X2Xb2 = XXb * XXb\n",
    "    sumXXb = np.sum(XXb)\n",
    "    sumX2Xb2 = np.sum(X2Xb2)\n",
    "    sumY = np.sum(Y)\n",
    "    sumXXbY = np.dot(XXb.T, Y)\n",
    "    A = np.array([[sumX2Xb2, sumXXb], [sumXXb, nS]])\n",
    "    b = np.array([sumXXbY, sumY])\n",
    "    coeff = np.linalg.lstsq(A, b, rcond=None)[0]\n",
    "    A = coeff[0]\n",
    "    C = coeff[1]\n",
    "    a2 = 2. * A * disc / (9. * D * D)\n",
    "    b2 = C + (A * E * E) / (9. * D * D) - (2. * A * F) / (3. * D)\n",
    "    a.a = np.array([a1, a2, a3])\n",
    "    a.b = np.array([b1, b2, b3])\n",
    "    tau = np.zeros((nS, 1))\n",
    "    for a.p in range(nS):\n",
    "        tau[a.p], _ = _solve_d_R_d_tau_p_3D(a)\n",
    "    return tau, a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415565c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_1D_open_manifold_3D(psi):\n",
    "    \"\"\"\n",
    "    Fits the eigenvectors for a 1D open manifold to a 3D model using the equation:\n",
    "    x_ij = a_j cos(j*pi*tau_i) + b_j, where 'j' ranges from 1 to 3 for 3D systems, and 'i' ranges from 1 to 'nS',\n",
    "    the number of data points to be fitted. This fitting process involves iterative optimization to find the \n",
    "    parameters 'a_j', 'b_j', and 'tau_i' that best fit the input data 'psi'.\n",
    "\n",
    "    Parameters:\n",
    "        psi (np.ndarray): A 2D numpy array containing the data points for the fitting process.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing three elements:\n",
    "               - np.ndarray: The optimized 'a' coefficients for the model.\n",
    "               - np.ndarray: The optimized 'b' coefficients for the model.\n",
    "               - np.ndarray: The optimized 'tau' values for each data point in the model.\n",
    "\n",
    "    Note:\n",
    "        - This function implements an iterative fitting procedure where for a fixed set of 'a_j' and 'b_j', 'tau_i'\n",
    "          are obtained by zeroing the derivative of 'R' with respect to 'tau_i'. Conversely, for a fixed set of 'tau_i',\n",
    "          'a_j' and 'b_j' are optimized by solving sets of linear equations.\n",
    "        - The fitting parameters and the initial set of 'tau' are specified in 'get_fit_1D_open_manifold_3D_param.m'.\n",
    "        - This implementation is adapted from the work of Russell Fung (2014) and translated into Python by\n",
    "          Columbia University Hstau Liao (2018).\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the fitting process encounters numerical issues or fails to converge within the specified\n",
    "                    number of iterations.\n",
    "    \"\"\"\n",
    "    tau, a = _get_fit_params(psi)\n",
    "    aux = np.zeros((tau.shape[0], 5))\n",
    "    nS = a.x.shape[0]\n",
    "    for iter in range(1, a.maxIter + 1):\n",
    "        # solve for a and b\n",
    "        a_old = a.a\n",
    "        b_old = a.b\n",
    "        j_pi_tau = np.dot(tau, np.pi * np.array([[1, 2, 3]]))\n",
    "        cos_j_pi_tau = np.cos(j_pi_tau)\n",
    "        A11 = np.sum(cos_j_pi_tau**2, axis=0)\n",
    "        A12 = np.sum(cos_j_pi_tau, axis=0)\n",
    "        A21 = A12\n",
    "        A22 = nS\n",
    "        x_cos_j_pi_tau = a.x * cos_j_pi_tau\n",
    "        b1 = np.sum(x_cos_j_pi_tau, axis=0)\n",
    "        b2 = np.sum(a.x, axis=0)\n",
    "        coeff = np.zeros((2, 3))\n",
    "        for qq in range(3):\n",
    "            A = np.array([[A11[qq], A12[qq]], [A21[qq], A22]])\n",
    "            b = np.array([b1[qq], b2[qq]])\n",
    "            coeff[:, qq] = np.linalg.lstsq(A, b, rcond=None)[0]\n",
    "\n",
    "        a.a = coeff[0, :]\n",
    "        a.b = coeff[1, :]\n",
    "        # plot the fitted curve (plotting functionality omitted for brevity)\n",
    "        j_pi_tau = np.dot(np.linspace(0, 1, 1000).reshape(-1, 1), np.array([[1, 2, 3]])) * np.pi\n",
    "        cos_j_pi_tau = np.cos(j_pi_tau)\n",
    "        tmp = a.a * cos_j_pi_tau\n",
    "        a.x_fit = tmp + a.b\n",
    "        # solve for tau\n",
    "        tau_old = tau\n",
    "        for a.p in range(nS):\n",
    "            tau[a.p], beta = _solve_d_R_d_tau_p_3D(a)\n",
    "            for kk in range(beta.shape[0]):\n",
    "                aux[a.p, kk] = beta[kk]\n",
    "        # calculate the changes in fitting parameters\n",
    "        delta_a = np.fabs(a.a - a_old) / (np.fabs(a.a) + eps)\n",
    "        delta_b = np.fabs(a.b - b_old) / (np.fabs(a.b) + eps)\n",
    "        delta_tau = np.fabs(tau - tau_old)\n",
    "        delta_a = max(delta_a) * 100\n",
    "        delta_b = max(delta_b) * 100\n",
    "        delta_tau = max(delta_tau)\n",
    "\n",
    "        if (delta_a < a.delta_a_max) and (delta_b < a.delta_b_max) and (delta_tau < a.delta_tau_max):\n",
    "            break\n",
    "    return (a.a, a.b, tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe475d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fergusonE(D, logEps, a0=None):\n",
    "    \"\"\"\n",
    "    Fits a curve using a hyperbolic tangent function to the data provided and returns the optimized parameters.\n",
    "\n",
    "    This function attempts to fit a curve defined by a hyperbolic tangent function to the given data points.\n",
    "    It uses the scipy.optimize.curve_fit method for curve fitting. The function also calculates the threshold\n",
    "    for weighting the data points based on the provided logarithmic epsilon values and distances.\n",
    "\n",
    "    Parameters:\n",
    "    - D (np.ndarray): An array of distances between data points.\n",
    "    - logEps (np.ndarray): An array of logarithmic epsilon values to be used for curve fitting.\n",
    "    - a0 (np.ndarray, optional): Initial guess for the parameters of the hyperbolic tangent function.\n",
    "      If None, a default value of ones(4) is used.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: A tuple containing the following elements:\n",
    "        - popt (np.ndarray): Optimal values for the parameters so that the sum of the squared residuals\n",
    "          of fun(xdata, *popt) - ydata is minimized.\n",
    "        - logSumWij (np.ndarray): Logarithm of the sum of weighted distances for each logEps value.\n",
    "        - resnorm (float): The sum of the square roots of the absolute values of the diagonal of the\n",
    "          covariance matrix of the parameters.\n",
    "        - R_squared (float): Coefficient of determination, indicating the proportion of the variance in\n",
    "          the dependent variable that is predictable from the independent variable(s).\n",
    "\n",
    "    Notes:\n",
    "    - The function internally defines a `fun` function representing a hyperbolic tangent model and a\n",
    "      `find_thres` function to calculate a threshold for weighting the data points.\n",
    "    - The curve fitting process iterates until the residual norm (resnorm) is less than 100, adjusting\n",
    "      the initial guess for the parameters (a0) in each iteration if necessary.\n",
    "    - This function uses scipy's curve_fit method, which may not converge to a solution; in such cases,\n",
    "      it prints the residual norm, the parameters attempted, and the error identifier (ier).\n",
    "    \"\"\"\n",
    "    if a0 is None:\n",
    "        a0 = np.ones(4)\n",
    "    def fun(x, a, b, c, d):\n",
    "        return d + c * np.tanh(a * x + b)\n",
    "    def find_thres(logEps, D2):\n",
    "        d = 0.5 * D2 / np.exp(np.max(logEps))\n",
    "        ss = np.sum(np.exp(-d))\n",
    "        return max(-np.log(0.01 * ss / len(D2)), 10)  # Taking 1% of the average (10)\n",
    "    # Range of values to try:\n",
    "    logSumWij = np.empty_like(logEps)\n",
    "    D2 = D * D\n",
    "    thr = find_thres(logEps, D2)\n",
    "    for k, le in enumerate(logEps):\n",
    "        d = 0.5 * D2 / np.exp(le)\n",
    "        Wij = np.exp(-d[d < thr])  # See Coifman 2008\n",
    "        logSumWij[k] = np.log(np.sum(Wij))\n",
    "    # Curve fitting of a tanh():\n",
    "    resnorm = np.inf\n",
    "    while (resnorm > 100):\n",
    "        popt, pcov, infodict, mesg, ier = curve_fit(fun, logEps, logSumWij, p0=a0, full_output=True)\n",
    "        resnorm = np.sum(np.sqrt(np.fabs(np.diag(pcov))))\n",
    "        if ier < 1 or ier > 4:\n",
    "            print(resnorm, popt, ier)\n",
    "        a0 *= 0.5\n",
    "        residuals = logSumWij - fun(logEps, *popt)\n",
    "        ss_res = np.sum(residuals**2)  # Residual sum of squares\n",
    "        ss_tot = np.sum((logSumWij - np.mean(logSumWij))**2)  # Total sum of squares\n",
    "        R_squared = 1 - (ss_res / ss_tot)  # R**2 value\n",
    "    return (popt, logSumWij, resnorm, R_squared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3416a2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DMembeddingII"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824cf478",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slaplacian(*arg):\n",
    "    \"\"\"\n",
    "    Given a set of nS data points, and the distances to nN nearest neighbors\n",
    "    for each data point, slaplacian computes a sparse, nY by nY symmetric\n",
    "    graph Laplacian matrix l.\n",
    "\n",
    "    The input data are supplied in the column vectors yVal and yInd of length\n",
    "    nY * nN such that\n",
    "\n",
    "    yVal( ( i - 1 ) * nN + ( 1 : nN ) ) contains the distances to the\n",
    "    nN nearest neighbors of data point i sorted in ascending order, and\n",
    "\n",
    "    yInd( ( i - 1 ) * nN + ( 1 : nN ) ) contains the indices of the nearest\n",
    "    neighbors.\n",
    "\n",
    "    yVal and yInd can be computed by calling nndist\n",
    "\n",
    "    slaplacian admits a number of options passed as name-value pairs\n",
    "\n",
    "    alpha : normalization, according to Coifman & Lafon\n",
    "\n",
    "    nAutotune : number of nearest neighbors for autotuning. Set to zero if no\n",
    "    autotuning is to be performed\n",
    "\n",
    "    sigma: width of the Gaussian kernel\n",
    "    \"\"\"\n",
    "    yVal = arg[0]\n",
    "    yCol = arg[1]\n",
    "    yRow = arg[2]\n",
    "    nS = arg[3]  # dataset size\n",
    "    options = arg[4]  # options.sigma: Gaussian width\n",
    "    nNZ = len(yVal)  # Number of nonzero elements\n",
    "\n",
    "    # If required, compute autotuning distances:\n",
    "    if options.autotune > 0:\n",
    "        print('Autotuning is not implemented in this version of slaplacian' + '\\n')\n",
    "    else:\n",
    "        sigmaTune = options.sigma\n",
    "\n",
    "    yVal = yVal / sigmaTune**2\n",
    "    # Compute the unnormalized weight matrix:\n",
    "    yVal = np.exp(-yVal)  # apply exponential weights (yVal is distance**2)\n",
    "    l = csc_matrix((yVal, (yRow, yCol)), shape=(nS, nS))\n",
    "    d = np.array(l.sum(axis=0)).T\n",
    "\n",
    "    if options.alpha != 1:  # apply non-isotropic normalization\n",
    "        d = d**options.alpha\n",
    "\n",
    "    yVal = yVal / (d[yRow].flatten('C') * d[yCol].flatten('C'))\n",
    "    l = csc_matrix((yVal, (yRow, yCol)), shape=(nS, nS))\n",
    "\n",
    "    # Normalize by the degree matrix to form normalized graph Laplacian:\n",
    "    d = np.array(l.sum(axis=0))\n",
    "    d = np.sqrt(d).T\n",
    "    yVal = yVal / (d[yRow].flatten('C') * d[yCol].flatten('C'))\n",
    "    l = csc_matrix((yVal, (yRow, yCol)), shape=(nS, nS))\n",
    "    l = np.abs(l + l.T) / 2.0  # iron out numerical wrinkles\n",
    "    temp = l - l.T\n",
    "\n",
    "    return (l, sigmaTune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d25a247",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sembedding(yVal, yCol, yRow, nS, options1):\n",
    "    \"\"\"\n",
    "    Laplacian eigenfunction embedding using sparse arrays.\n",
    "\n",
    "    This function computes the eigenvalues and eigenvectors of the Laplacian matrix of a graph,\n",
    "    which represents the dataset. The Laplacian matrix is constructed based on the input sparse\n",
    "    matrix components (values, column indices, row pointers) and options specifying the embedding\n",
    "    parameters.\n",
    "\n",
    "    Parameters:\n",
    "    - yVal (np.ndarray): The values of the non-zero elements in the sparse matrix representation.\n",
    "    - yCol (np.ndarray): The column indices of the non-zero elements in the sparse matrix.\n",
    "    - yRow (np.ndarray): The row indices of the non-zero elements in the sparse matrix.\n",
    "    - nS (int): The number of samples or nodes in the graph.\n",
    "    - options1 (namedtuple): A namedtuple containing the options for the embedding. Expected fields\n",
    "      are sigma (float), alpha (float), visual (bool), nEigs (int), and autotune (int).\n",
    "\n",
    "    Returns:\n",
    "    - tuple: A tuple containing the following elements:\n",
    "        - vals (np.ndarray): The computed eigenvalues of the Laplacian matrix.\n",
    "        - vecs (np.ndarray): The computed eigenvectors of the Laplacian matrix, corresponding to the eigenvalues.\n",
    "\n",
    "    Notes:\n",
    "    - The function attempts to compute the specified number of eigenvalues and eigenvectors using\n",
    "      the ARPACK solver via scipy.sparse.linalg.eigsh. If the solver does not converge within the\n",
    "      specified maximum number of iterations, it catches the ArpackNoConvergence exception and\n",
    "      returns the eigenvalues and eigenvectors that were computed up to that point.\n",
    "    \"\"\"\n",
    "    # Create a new options namedtuple with values from options1\n",
    "    options = namedtuple('options', 'sigma alpha visual nEigs autotune')\n",
    "    options.sigma = options1.sigma\n",
    "    options.alpha = options1.alpha\n",
    "    options.nEigs = options1.nEigs\n",
    "    options.autotune = 0  # Autotuning is disabled in this implementation\n",
    "\n",
    "    # Compute the sparse Laplacian matrix\n",
    "    l, sigmaTune = slaplacian(yVal, yCol, yRow, nS, options)\n",
    "\n",
    "    # Attempt to compute the eigenvalues and eigenvectors of the Laplacian matrix\n",
    "    try:\n",
    "        vals, vecs = eigsh(l, k=options.nEigs + 1, maxiter=300, v0=np.ones(nS), return_eigenvectors=True)\n",
    "    except ArpackNoConvergence as e:\n",
    "        # Handle the case where ARPACK does not converge within the maximum number of iterations\n",
    "        vals = e.eigenvalues\n",
    "        vecs = e.eigenvectors\n",
    "        print(\"eigsh not converging in 300 iterations...\")\n",
    "\n",
    "    # Sort the eigenvalues and eigenvectors in descending order\n",
    "    ix = np.argsort(vals)[::-1]\n",
    "    vals = vals[ix]\n",
    "    vecs = vecs[:, ix]\n",
    "\n",
    "    return (vals, vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb1b285",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_yColVal(params):\n",
    "    \"\"\"\n",
    "    Processes and updates arrays for values and column indices in a sparse matrix representation.\n",
    "\n",
    "    This function takes a set of parameters related to the sparse matrix construction, including\n",
    "    arrays of values and indices, and updates these arrays based on the provided batch of data.\n",
    "    It is typically used in the context of constructing or updating a sparse representation of a\n",
    "    graph or matrix, especially when dealing with large datasets that require batching.\n",
    "\n",
    "    Parameters:\n",
    "    - params (tuple): A tuple containing the following elements:\n",
    "        - yVal (np.ndarray): The values of the non-zero elements in the sparse matrix.\n",
    "        - yVal1 (np.ndarray): The original values from which yVal will be updated.\n",
    "        - yCol (np.ndarray): The column indices of the non-zero elements in the sparse matrix.\n",
    "        - yInd1 (np.ndarray): The original indices from which yCol will be updated.\n",
    "        - nB (int): The batch size, indicating the number of data points processed in this batch.\n",
    "        - nN (int): The number of nearest neighbors considered for each data point.\n",
    "        - nNIn (int): The initial number of nearest neighbors before filtering.\n",
    "        - jStart (int): The start index of the current batch.\n",
    "        - jEnd (int): The end index of the current batch.\n",
    "        - indStart (int): The start index for updating yVal and yCol.\n",
    "        - indEnd (int): The end index for updating yVal and yCol.\n",
    "        - iBatch (int): The current batch number.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: A tuple containing the updated yCol and yVal arrays.\n",
    "\n",
    "    Notes:\n",
    "    - The function reshapes and filters the input arrays yVal1 and yInd1 based on the batch\n",
    "      information and the number of nearest neighbors. It then updates the yVal and yCol arrays\n",
    "      with the processed data for the current batch.\n",
    "    - This function is part of a larger process of constructing or updating sparse matrices and\n",
    "      is designed to handle data in batches for efficiency and scalability.\n",
    "    \"\"\"\n",
    "    yVal = params[0]\n",
    "    yVal1 = params[1]\n",
    "    yCol = params[2]\n",
    "    yInd1 = params[3]\n",
    "    nB = params[4]\n",
    "    nN = params[5]\n",
    "    nNIn = params[6]\n",
    "    jStart = params[7]\n",
    "    jEnd = params[8]\n",
    "    indStart = params[9]\n",
    "    indEnd = params[10]\n",
    "    iBatch = params[11]\n",
    "\n",
    "    # Reshape and filter the data batch for values\n",
    "    DataBatch = yVal1\n",
    "    DataBatch = DataBatch.reshape(nB, nNIn).T\n",
    "    DataBatch = DataBatch[:nN, :]\n",
    "    DataBatch[0, :] = 0  # Reset the first row to zeros\n",
    "    yVal[indStart:indEnd] = DataBatch.reshape(nN * nB, 1)\n",
    "\n",
    "    # Reshape and filter the data batch for column indices\n",
    "    DataBatch = yInd1\n",
    "    DataBatch = DataBatch.reshape(nB, nNIn).T\n",
    "    DataBatch = DataBatch[:nN, :]\n",
    "    yCol[indStart:indEnd] = DataBatch.reshape(nN * nB, 1).astype(float)\n",
    "\n",
    "    return (yCol, yVal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8cbb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(nS, nN, D):\n",
    "    \"\"\"\n",
    "    Initializes the arrays of indices and values for constructing a sparse matrix\n",
    "    representation of distances between data points.\n",
    "\n",
    "    This function processes a distance matrix to identify the nearest neighbors for each\n",
    "    data point. It then creates arrays that store the indices of these neighbors and the\n",
    "    corresponding distance values. The first distance value for each data point is set to\n",
    "    zero to indicate self-distance, ensuring the diagonal of the distance matrix is zero.\n",
    "\n",
    "    Parameters:\n",
    "    - nS (int): The number of samples or data points.\n",
    "    - nN (int): The number of nearest neighbors to consider for each data point.\n",
    "    - D (np.ndarray): A square distance matrix of shape (nS, nS) where D[i, j] represents\n",
    "      the distance between the i-th and j-th data points.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: A tuple containing two flattened arrays:\n",
    "        - yInd1 (np.ndarray): A flattened array of indices of the nearest neighbors for each data point.\n",
    "        - yVal1 (np.ndarray): A flattened array of the corresponding distance values to the nearest neighbors.\n",
    "\n",
    "    Notes:\n",
    "    - The function modifies the input distance matrix D by setting the diagonal elements to\n",
    "      negative infinity to ensure that each data point's self-distance does not affect the\n",
    "      nearest neighbors' calculation.\n",
    "    - After identifying the nearest neighbors and their distances, the function resets the\n",
    "      first distance value for each data point to zero, effectively ignoring self-distance\n",
    "      in the sparse matrix representation.\n",
    "    \"\"\"\n",
    "    yInd1 = np.zeros((nN, nS), dtype='int32')\n",
    "    yVal1 = np.zeros((nN, nS), dtype='float64')\n",
    "    \n",
    "    for iS in range(nS):\n",
    "        D[iS, iS] = -np.Inf  # Force this distance to be the minimal value\n",
    "        B = np.sort(D[:, iS])\n",
    "        IX = np.argsort(D[:, iS])\n",
    "        yInd1[:, iS] = IX[:nN]\n",
    "        yVal1[:, iS] = B[:nN]\n",
    "        yVal1[0, iS] = 0  # Set this distance back to zero\n",
    "    \n",
    "    yInd1 = yInd1.flatten('F')\n",
    "    yVal1 = yVal1.flatten('F')\n",
    "    \n",
    "    return (yInd1, yVal1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef563986",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_matrix0(Row, Col, Val, nZ, nS):\n",
    "    \"\"\"\n",
    "    Constructs a symmetric matrix from given row indices, column indices, and values,\n",
    "    specifically designed for handling squared distances.\n",
    "\n",
    "    This function first creates a sparse matrix from the given row indices, column indices,\n",
    "    and values. It then converts this sparse matrix to a dense array and performs operations\n",
    "    to ensure that the resulting matrix is symmetric and represents squared distances\n",
    "    correctly.\n",
    "\n",
    "    Parameters:\n",
    "    - Row (np.ndarray): An array of row indices for the non-zero elements in the matrix.\n",
    "    - Col (np.ndarray): An array of column indices for the non-zero elements in the matrix.\n",
    "    - Val (np.ndarray): An array of values corresponding to the non-zero elements in the matrix.\n",
    "    - nZ (int): The number of zero elements in the matrix. This parameter is accepted but not\n",
    "      directly used in the function, indicating potential use in extended functionality or\n",
    "      error checking.\n",
    "    - nS (int): The size of the square matrix to be constructed, indicating both the number\n",
    "      of rows and columns.\n",
    "\n",
    "    Returns:\n",
    "    - np.ndarray: A symmetric matrix constructed from the input parameters, with adjustments\n",
    "      to ensure correct representation of squared distances.\n",
    "\n",
    "    Notes:\n",
    "    - The function first constructs a sparse CSR (Compressed Sparse Row) matrix from the input\n",
    "      indices and values. It then converts this sparse matrix to a dense array.\n",
    "    - It computes the square of the dense array and its transpose to handle squared distances.\n",
    "    - The final matrix is adjusted by adding the original matrix and its transpose, then\n",
    "      subtracting a matrix that contains the squares of the distances, to ensure symmetry\n",
    "      and correct distance representation.\n",
    "    \"\"\"\n",
    "    # Create a sparse CSR matrix from the given indices and values\n",
    "    y = csr_matrix((Val, (Row, Col)), shape=(nS, nS)).toarray()\n",
    "    \n",
    "    # Compute the square of the dense array and its transpose\n",
    "    y2 = y * y.T  # y2 contains the squares of the distances\n",
    "    \n",
    "    # Square the original matrix\n",
    "    y = y**2\n",
    "    \n",
    "    # Adjust the matrix to ensure symmetry and correct representation of squared distances\n",
    "    y = y + y.T - y2\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588eb0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_matrix1(Row, Col, Val, nZ, nS):\n",
    "    \"\"\"\n",
    "    Constructs a symmetric matrix from given row indices, column indices, and values.\n",
    "    This version simplifies the process by directly adjusting for symmetry without explicitly\n",
    "    squaring the matrix values.\n",
    "\n",
    "    Parameters:\n",
    "    - Row (np.ndarray): An array of row indices for the non-zero elements in the matrix.\n",
    "    - Col (np.ndarray): An array of column indices for the non-zero elements in the matrix.\n",
    "    - Val (np.ndarray): An array of values corresponding to the non-zero elements in the matrix.\n",
    "    - nZ (int): The number of zero elements in the matrix. This parameter is accepted but not\n",
    "      directly used in the function, indicating potential use in extended functionality or\n",
    "      error checking.\n",
    "    - nS (int): The size of the square matrix to be constructed, indicating both the number\n",
    "      of rows and columns.\n",
    "\n",
    "    Returns:\n",
    "    - np.ndarray: A symmetric matrix constructed from the input parameters. The symmetry is\n",
    "      achieved by adding the matrix to its transpose and then subtracting the element-wise\n",
    "      product of the matrix and its transpose.\n",
    "\n",
    "    Notes:\n",
    "    - The function constructs a sparse CSR (Compressed Sparse Row) matrix from the input indices\n",
    "      and values and then converts this sparse matrix to a dense array.\n",
    "    - It ensures the symmetry of the resulting matrix by adding it to its transpose and\n",
    "      subtracting the element-wise product of the matrix and its transpose, which contains\n",
    "      the squares of the original distances. This operation corrects for any asymmetries\n",
    "      that might arise from the input data or the sparse matrix construction process.\n",
    "    \"\"\"\n",
    "    # Create a sparse CSR matrix from the given indices and values\n",
    "    y = csr_matrix((Val, (Row, Col)), shape=(nS, nS)).toarray()\n",
    "    \n",
    "    # Compute the element-wise product of the matrix and its transpose\n",
    "    y2 = y * y.T  # y2 contains the squares of the distances\n",
    "    \n",
    "    # Adjust the matrix to ensure symmetry\n",
    "    y = y + y.T - y2\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d57f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DMembeddingIIop(D, k, tune, prefsigma):\n",
    "    \"\"\"\n",
    "    Performs spectral embedding of data points based on their pairwise distances using\n",
    "    the diffusion maps approach. This function initializes the necessary structures,\n",
    "    processes the data to construct a symmetric distance matrix, and performs spectral\n",
    "    decomposition to obtain embedding coordinates. Additionally, it estimates the optimal\n",
    "    sigma for the Gaussian kernel using the Ferguson method.\n",
    "\n",
    "    Parameters:\n",
    "    - D (np.ndarray): A square matrix of pairwise distances between data points.\n",
    "    - k (int): The number of nearest neighbors to consider.\n",
    "    - tune (float): A tuning parameter for adjusting the Gaussian kernel width.\n",
    "    - prefsigma (float): A predefined sigma value for the Gaussian kernel, used if the\n",
    "      tuning process based on the Ferguson method does not converge.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: A tuple containing the following elements:\n",
    "        - lamb (np.ndarray): The computed eigenvalues of the Laplacian matrix.\n",
    "        - psi (np.ndarray): The computed eigenvectors of the Laplacian matrix, representing\n",
    "          the embedding coordinates.\n",
    "        - sigma (float): The final sigma value used for the Gaussian kernel.\n",
    "        - mu (np.ndarray): The Riemannian measure derived from the first eigenvector.\n",
    "        - logEps (np.ndarray): The logarithmic epsilon values used in the Ferguson method.\n",
    "        - logSumWij (np.ndarray): The logarithm of the sum of weights for each logEps value.\n",
    "        - popt (np.ndarray): The optimized parameters from the Ferguson curve fitting.\n",
    "        - R_squared (float): The coefficient of determination from the curve fitting.\n",
    "\n",
    "    Notes:\n",
    "    - The function begins by initializing nearest neighbors and constructing a symmetric\n",
    "      distance matrix through batch processing.\n",
    "    - It employs the Ferguson method to estimate the optimal sigma for the Gaussian kernel\n",
    "      based on the distance matrix.\n",
    "    - Spectral decomposition of the Laplacian matrix is performed to obtain the embedding\n",
    "      coordinates (psi) and the eigenvalues (lamb).\n",
    "    - The process includes a fallback to a predefined sigma (prefsigma) if the tuning does\n",
    "      not converge within a specified threshold.\n",
    "    - The Riemannian measure (mu) is calculated as the square of the first eigenvector,\n",
    "      normalized so that its sum equals 1. This measure can be used to weight the embedded\n",
    "      points in further analysis.\n",
    "    \"\"\"\n",
    "    nS = D.shape[0]\n",
    "    nN = k  # Total number of entries is nS*nN\n",
    "    yInd1, yVal1 = initialize(nS, nN, D)\n",
    "\n",
    "    # Diffraction patterns:\n",
    "    nB = nS  # Batch size (number of diff. patterns per batch)\n",
    "    nNIn = k  # Number of input nearest neighbors\n",
    "    nN = k  # Number of output nearest neighbors\n",
    "    yVal = np.zeros((nS * nN, 1))\n",
    "    yCol = np.zeros((nS * nN, 1))\n",
    "    nBatch = int(nS / nB)\n",
    "\n",
    "    for iBatch in range(nBatch):\n",
    "        # Linear indices in the non-symmetric distance matrix (indStart, indEnd)\n",
    "        indStart = iBatch * nB * nN\n",
    "        indEnd = (iBatch + 1) * nB * nN\n",
    "        # Diffraction pattern indices (jStart, jEnd):\n",
    "        jStart = iBatch * nB\n",
    "        jEnd = (iBatch + 1) * nB\n",
    "        params = (yVal, yVal1, yCol, yInd1, nB, nN, nNIn, jStart, jEnd, indStart, indEnd, iBatch)\n",
    "        yCol, yVal = get_yColVal(params)\n",
    "\n",
    "    # Symmetrizing the distance matrix\n",
    "    yRow = np.ones((nN, 1)) * range(nS)\n",
    "    yRow = yRow.reshape(nS * nN, 1)\n",
    "    ifZero = yVal < 1e-6\n",
    "    yRowNZ = yRow[~ifZero]\n",
    "    yColNZ = yCol[~ifZero]\n",
    "    yValNZ = np.sqrt(yVal[~ifZero])\n",
    "    nNZ = len(yRowNZ)  # Number of nonzero elements in the non-sym matrix\n",
    "    yRow = yRow[ifZero]\n",
    "    yCol = yCol[ifZero]\n",
    "    nZ = len(yRow)  # Number of zero elements in the non-sym matrix\n",
    "    y = construct_matrix0(yRowNZ, yColNZ, yValNZ, nZ, nS)\n",
    "    yRowNZ = y.nonzero()[0]\n",
    "    yColNZ = y.nonzero()[1]\n",
    "    yValNZ = y[y.nonzero()]\n",
    "    nNZ = len(y.nonzero()[0]) # Number of nonzero elements in the sym matrix\n",
    "    y = construct_matrix1(yRow, yCol, np.ones((nZ, 1)).flatten(), nZ, nS)\n",
    "    y = csr_matrix((np.ones((nZ, 1)).flatten(), (yRow, yCol)), shape=(nS, nS)).toarray()\n",
    "    yRow = y.nonzero()[0]\n",
    "    yCol = y.nonzero()[1]\n",
    "    yVal = y[y.nonzero()]\n",
    "    yVal[:] = 0\n",
    "    nZ = len(y.nonzero()[0])  #number of zero elements in the sym matrix\n",
    "    yRow = np.hstack((yRow, yRowNZ)).astype(int)\n",
    "    yCol = np.hstack((yCol, yColNZ)).astype(int)\n",
    "    yVal = np.hstack((yVal, yValNZ))\n",
    "    count = 0\n",
    "    resnorm = np.inf\n",
    "    logEps = np.arange(-150, 150.2, 0.2)\n",
    "    popt, logSumWij, resnorm, R_squared = fergusonE(np.sqrt(yVal), logEps)\n",
    "    nS = D.shape[0]\n",
    "    nEigs = min(p.num_eigs, nS - 3)  # Number of eigenfunctions to compute\n",
    "    nA = 0  # Autotuning parameter\n",
    "    nN = k  # Number of nearest neighbors\n",
    "    nNA = 0  # Number of nearest neighbors used for autotuning\n",
    "    if count < 20:\n",
    "        alpha = 1  # Kernel normalization\n",
    "        sigma = tune * np.sqrt(2 * np.exp(-popt[1] / popt[0]))  # Gaussian Kernel width (=1 for autotuning)\n",
    "    else:\n",
    "        print('using prefsigma...')  # Fallback to predefined sigma\n",
    "        sigma = prefsigma\n",
    "        alpha = 1\n",
    "    visual = 1\n",
    "    options = namedtuple('Options', 'sigma alpha visual nEigs')\n",
    "    options.sigma = sigma\n",
    "    options.alpha = alpha\n",
    "    options.visual = visual\n",
    "    options.nEigs = nEigs\n",
    "    lamb, v = sembedding(yVal, yCol, yRow, nS, options)\n",
    "    true_shape = v.shape[1] - 1\n",
    "    psi = np.zeros((v.shape[0], nEigs))\n",
    "    psi[:, :true_shape] = v[:, 1:] / np.tile(v[:, 0].reshape((-1, 1)), (1, true_shape))\n",
    "    mu = v[:, 0]\n",
    "    mu = mu * mu  # The Riemannian measure\n",
    "    return (lamb, psi, sigma, mu, logEps, logSumWij, popt, R_squared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0374441",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _corr(a, b, n, m):\n",
    "    \"\"\"\n",
    "    Calculates the correlation coefficient between two vectors extracted from matrices 'a' and 'b'.\n",
    "\n",
    "    The function selects vectors from the n-th column of matrix 'a' and the m-th column of matrix 'b',\n",
    "    normalizes these vectors by subtracting their means, and computes the correlation coefficient\n",
    "    between the normalized vectors.\n",
    "\n",
    "    Parameters:\n",
    "        a (np.ndarray): A 2D numpy array from which the first vector is selected.\n",
    "        b (np.ndarray): A 2D numpy array from which the second vector is selected.\n",
    "        n (int): The column index in 'a' from which the vector is selected.\n",
    "        m (int): The column index in 'b' from which the vector is selected.\n",
    "\n",
    "    Returns:\n",
    "        float: The correlation coefficient between the two selected vectors.\n",
    "\n",
    "    Raises:\n",
    "        RuntimeError: If the computation encounters a \"flat image\" scenario where the standard\n",
    "        deviation of the vectors becomes zero, leading to a division by zero in the correlation\n",
    "        calculation.\n",
    "\n",
    "    Note:\n",
    "        This function is sensitive to \"flat\" vectors where there's no variation in the data, as\n",
    "        indicated by the zero standard deviation check that could raise a RuntimeError.\n",
    "    \"\"\"\n",
    "    A = a[:, n]\n",
    "    B = b[:, m]\n",
    "    A = A - np.mean(A)\n",
    "    B = B - np.mean(B)\n",
    "    try:\n",
    "        co = np.dot(A, B) / (np.std(A) * np.std(B))\n",
    "    except RuntimeError:\n",
    "        raise RuntimeError(\"flat image\")\n",
    "    return co"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebfa728",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _diff_corr(a, b, maxval):\n",
    "    \"\"\"\n",
    "    Computes a differential correlation metric between two matrices 'a' and 'b'.\n",
    "\n",
    "    This metric is derived by calculating the correlation between corresponding vectors in 'a'\n",
    "    and 'b' at indices 0 and 'maxval', then finding the difference between these correlations\n",
    "    and the cross correlations at indices (0, maxval) and (maxval, 0). This method can highlight\n",
    "    differences in correlation patterns across the matrices, useful in comparative analysis.\n",
    "\n",
    "    Parameters:\n",
    "        a (np.ndarray): The first 2D numpy array for correlation calculation.\n",
    "        b (np.ndarray): The second 2D numpy array for correlation calculation.\n",
    "        maxval (int): The maximum column index used for calculating differential correlation.\n",
    "\n",
    "    Returns:\n",
    "        float: The differential correlation metric calculated across the specified indices\n",
    "        of matrices 'a' and 'b'.\n",
    "\n",
    "    Example:\n",
    "        If 'a' and 'b' represent matrices with temporal data across different conditions, this\n",
    "        metric can indicate changes in correlation structure between these conditions at\n",
    "        specific points in time or space.\n",
    "    \"\"\"\n",
    "    return _corr(a, b, 0, 0) + _corr(a, b, maxval, maxval) - \\\n",
    "        (_corr(a, b, 0, maxval) + _corr(a, b, maxval, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7419c521",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _NLSA(NLSAPar, DD, posPath, posPsi1, imgAll, msk2, CTF, ExtPar):\n",
    "    \"\"\"\n",
    "    Implements the Nonlinear Local Spectral Analysis (NLSA) algorithm to analyze and reconstruct\n",
    "    data from a manifold structure. This complex function integrates manifold learning, signal reconstruction,\n",
    "    and image processing techniques to process a given dataset 'DD'.\n",
    "\n",
    "    Parameters:\n",
    "        NLSAPar (dict): A dictionary containing parameters for the NLSA algorithm, such as 'num', 'ConOrder', 'k',\n",
    "                        'tune', 'nS', and 'psiTrunc'.\n",
    "        DD (np.ndarray): A distance matrix representing the distances between data points.\n",
    "        posPath (list): A list of positions in the path of data points.\n",
    "        posPsi1 (int): The position of Psi1 in the dataset.\n",
    "        imgAll (np.ndarray): An array of all images to be processed.\n",
    "        msk2 (np.ndarray): A mask to be applied to the images.\n",
    "        CTF (np.ndarray): The Contrast Transfer Function used for image processing.\n",
    "        ExtPar (dict): A dictionary containing extra parameters for processing, such as 'prD' and 'cuti'.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing multiple elements, including the reconstructed image matrix 'IMGT', the mean\n",
    "               topological features 'Topo_mean', the reconstructed eigenvectors 'psirec', the original eigenvectors\n",
    "               'psiC1', singular values 'sdiag', the transformed eigenvectors 'VX', the means 'mu', and the\n",
    "               temporal parameters 'tau'.\n",
    "\n",
    "    Note:\n",
    "        - The function computes a series of complex steps including Wiener filtering, Singular Value Decomposition\n",
    "          (SVD), and Diffusion Maps embedding to achieve the reconstruction and analysis.\n",
    "        - The process involves fitting a manifold to a model, rearranging arrays based on external parameters,\n",
    "          applying image filters, and ultimately reconstructing the data using optimized parameters.\n",
    "        - This implementation also includes error handling for imaginary matrices and normalization of reconstructed\n",
    "          frames to have a mean of 0 and a standard deviation of 1.\n",
    "        - The final step involves the use of the 'fit_1D_open_manifold_3D' function to fit the eigenvectors for a 1D\n",
    "          open manifold to the model, and potentially saving the results if specified in 'NLSAPar'.\n",
    "        - The algorithm is sensitive to the input parameters and the structure of the input data, requiring careful\n",
    "          preparation of the data and tuning of parameters for optimal results.\n",
    "\n",
    "    Raises:\n",
    "        SystemExit: If the matrix 'A' turns out to be imaginary, indicating a failure in the reconstruction process,\n",
    "                    or if there is an attempt to normalize a \"flat image\" where the standard deviation is zero.\n",
    "    \"\"\"\n",
    "    num = NLSAPar['num']\n",
    "    ConOrder = NLSAPar['ConOrder']\n",
    "    k = NLSAPar['k']\n",
    "    tune = NLSAPar['tune']\n",
    "    nS = NLSAPar['nS']\n",
    "    psiTrunc = NLSAPar['psiTrunc']\n",
    "    ConD = np.zeros((num - ConOrder, num - ConOrder))\n",
    "    for i in range(ConOrder):\n",
    "        Ind = range(i, num - ConOrder + i)\n",
    "        ConD += DD[Ind][:, Ind]\n",
    "    # find the manifold mapping\n",
    "    lambdaC, psiC, sigmaC, mu, logEps, logSumWij, popt, R_squared = DMembeddingIIop(\n",
    "        ConD, k, tune, 600000)  # USE THE MU FROM SUPERVECTORS' DISTANCES\n",
    "    lambdaC = lambdaC[lambdaC > 0]  # lambdaC not used? REVIEW\n",
    "    psiC1 = np.copy(psiC)\n",
    "    # rearrange arrays\n",
    "    if 'prD' in ExtPar:\n",
    "        IMG1 = imgAll[posPath[posPsi1], :, :]\n",
    "        # Wiener filtering\n",
    "        wiener_dom, CTF1 = get_wiener(CTF, posPath, posPsi1, ConOrder, num)\n",
    "    elif 'cuti' in ExtPar:\n",
    "        IMG1 = imgAll[posPsi1, :, :]\n",
    "    dim = CTF.shape[1]\n",
    "    ell = psiTrunc - 1\n",
    "    N = psiC.shape[0]\n",
    "    psiC = np.hstack((np.ones((N, 1)), psiC[:, 0:ell]))\n",
    "    mu_psi = mu.reshape((-1, 1)) * psiC\n",
    "    A = np.zeros((ConOrder * dim * dim, ell + 1), dtype='float64')\n",
    "    tmp = np.zeros((dim * dim, num - ConOrder), dtype='float64')\n",
    "    for ii in range(ConOrder):\n",
    "        for i in range(num - ConOrder):\n",
    "            ind1 = 0\n",
    "            ind2 = dim * dim\n",
    "            ind3 = ConOrder - ii + i - 1\n",
    "            img = IMG1[ind3, :, :]\n",
    "            if 'prD' in ExtPar:\n",
    "                img_f = fft2(img)\n",
    "                CTF_i = CTF1[ind3, :, :]\n",
    "                img_f_wiener = img_f * (CTF_i / wiener_dom[i, :, :])\n",
    "                img = ifft2(img_f_wiener).real\n",
    "                img = img * msk2\n",
    "            tmp[ind1:ind2, i] = np.squeeze(img.T.reshape(-1, 1))\n",
    "        mm = dim * dim\n",
    "        ind4 = ii * mm\n",
    "        ind5 = ind4 + mm\n",
    "        A[ind4:ind5, :] = np.matmul(tmp, mu_psi)\n",
    "    TF = np.isreal(A)\n",
    "    if TF.any() != True:\n",
    "        print('A is an imaginary matrix!')\n",
    "        sys.exit()\n",
    "    U, S, V = svdRF(A)\n",
    "    VX = np.matmul(V.T, psiC.T)\n",
    "    sdiag = np.diag(S)\n",
    "    Npixel = dim * dim\n",
    "    Topo_mean = np.zeros((Npixel, psiTrunc))\n",
    "    for ii in range(psiTrunc):\n",
    "        Topo = np.ones((Npixel, ConOrder)) * np.Inf\n",
    "        for k in range(ConOrder):\n",
    "            Topo[:, k] = U[k * Npixel:(k + 1) * Npixel, ii]\n",
    "        Topo_mean[:, ii] = np.mean(Topo, axis=1)\n",
    "    i2 = 1\n",
    "    i1 = 0\n",
    "    ConImgT = np.zeros((max(U.shape), ell + 1), dtype='float64')\n",
    "    for i in range(i1, i2 + 1):\n",
    "        ConImgT = ConImgT + np.matmul(U[:, i].reshape(-1, 1), sdiag[i] * (V[:, i].reshape(1, -1)))\n",
    "    recNum = ConOrder\n",
    "    IMGT = np.zeros((Npixel, nS - ConOrder - recNum), dtype='float64')\n",
    "    for i in range(recNum):\n",
    "        ind1 = i * Npixel\n",
    "        ind2 = ind1 + Npixel\n",
    "        tmp = np.matmul(ConImgT[ind1:ind2, :], psiC.T)\n",
    "        for ii in range(num - 2 * ConOrder):\n",
    "            ind3 = i + ii\n",
    "            ttmp = IMGT[:, ii]\n",
    "            ttmp = ttmp + tmp[:, ind3]\n",
    "            IMGT[:, ii] = ttmp\n",
    "    for i in range(IMGT.shape[1]):\n",
    "        ttmp = IMGT[:, i]\n",
    "        try:\n",
    "            ttmp = (ttmp - np.mean(ttmp)) / np.std(ttmp)\n",
    "        except:\n",
    "            print(\"flat image\")\n",
    "            exit(0)\n",
    "        IMGT[:, i] = ttmp\n",
    "    nSrecon = min(IMGT.shape)\n",
    "    Drecon = L2_distance(IMGT, IMGT)\n",
    "    k = nSrecon\n",
    "    lamb, psirec, sigma, mu, logEps, logSumWij, popt, R_squared = DMembeddingIIop((Drecon**2), k, tune, 30)\n",
    "    lamb = lamb[lamb > 0]\n",
    "    a, b, tau = fit_1D_open_manifold_3D(psirec)\n",
    "    if NLSAPar['save'] is True:\n",
    "        fout1(ExtPar['filename'], psirec=psirec, tau=tau, a=a, b=b)\n",
    "    return (IMGT, Topo_mean, psirec, psiC1, sdiag, VX, mu, tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569a9be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def psi_analysis_single(input_data, con_order_range, traj_name, is_full, psi_trunc):\n",
    "    \"\"\"\n",
    "    Performs Psi Analysis on a single dataset, characterizing the manifold structure\n",
    "    and dynamics within cryo-EM data by analyzing the diffusion map coordinates.\n",
    "    \n",
    "    Parameters:\n",
    "        input_data (list): Contains paths to input files, psi numbers, senses, and other necessary parameters.\n",
    "        con_order_range (int): Defines the range for conformation order calculations.\n",
    "        traj_name (str): Name of the trajectory for result identification.\n",
    "        is_full (bool): Indicates whether to perform a full analysis.\n",
    "        psi_trunc (int): Specifies the truncation level for psi dimensions.\n",
    "    \n",
    "    The function integrates various steps:\n",
    "    - Loading and preprocessing of distance matrices and image data.\n",
    "    - Organizing and sorting data based on psi coordinates for manifold analysis.\n",
    "    - Executing Nonlinear Laplacian Spectral Analysis (NLSA) to identify dynamic states.\n",
    "    - Classifying images into classes based on their psi coordinates and analyzing transitions.\n",
    "    - Optionally, adjusting tau values and performing extended analysis.\n",
    "    \n",
    "    Results include classified images, manifold coordinates, and adjusted tau values, providing\n",
    "    insights into the underlying dynamics and structural variations within the dataset.\n",
    "    \"\"\"\n",
    "    # Extracting individual elements from the input data array.\n",
    "    dist_file = input_data[0]  # Path to the file containing the distance matrix.\n",
    "    psi_file = input_data[1]  # File containing 15-dimensional diffusion map coordinates.\n",
    "    psi2_file = input_data[2]  # Target file path for the output of Psi Analysis.\n",
    "    EL_file = input_data[3]  # File path for eigenvalues list.\n",
    "    psinums = input_data[4]  # Array of psi numbers to be analyzed.\n",
    "    senses = input_data[5]  # Array indicating the directionality (sense) of each psi number.\n",
    "    prD = input_data[6]  # Processing job identifier.\n",
    "    \n",
    "    # Handling an optional eighth element in input data for a custom list of psi values.\n",
    "    psi_list = input_data[7] if len(input_data) == 8 else psinums\n",
    "    \n",
    "    # Loading data from specified files.\n",
    "    data_IMG = fin1(dist_file)  # Load image data, including distance matrix and image matrix.\n",
    "    data_psi = fin1(psi_file)  # Load psi data, including psi coordinates and positional paths.\n",
    "    \n",
    "    # Preprocessing and setup before analysis.\n",
    "    D = np.array(data_IMG['D'])  # Convert distance matrix to numpy array.\n",
    "    imgAll = np.array(data_IMG['imgAll'])  # Convert images matrix to numpy array.\n",
    "    msk2 = np.array(data_IMG['msk2'])  # Volume mask to be used after CTF application.\n",
    "    CTF = np.array(data_IMG['CTF'])  # CTF values for each image.\n",
    "    psi = data_psi['psi']  # Psi coordinates for images.\n",
    "    pos_path = data_psi['posPath']  # Indices of images in the path.\n",
    "    nS = len(pos_path)  # Total number of images.\n",
    "    con_order = nS // con_order_range  # Determination of conformation order.\n",
    "    \n",
    "    # Reshaping and reorganizing data for analysis.\n",
    "    dim = int(np.sqrt(imgAll.size / D.shape[0]))  # Calculate dimension size.\n",
    "    CTF = CTF.reshape(D.shape[0], dim, dim)  # Reshape CTF values.\n",
    "    pos_path = np.squeeze(pos_path)  # Remove single-dimensional entries from pos_path.\n",
    "    D = D[pos_path][:, pos_path]  # Rearrange distance matrix according to pos_path.\n",
    "    \n",
    "    # Setup extra parameters for analysis.\n",
    "    extra_params = dict(outDir='', prD=prD)\n",
    "    \n",
    "    # Main loop for processing each psi number.\n",
    "    for psinum in psi_list:\n",
    "        if psinum == -1:\n",
    "            continue  # Skip processing if psi number is marked as -1 (invalid or to be ignored).\n",
    "        \n",
    "        psi_sorted_ind = np.argsort(psi[:, psinum])  # Sort indices based on current psi number.\n",
    "        pos_psi1 = psi_sorted_ind  # Use sorted indices for reordering images.\n",
    "        \n",
    "        # Reorganize distance matrix based on sorted psi indices.\n",
    "        DD = D[pos_psi1][:, pos_psi1]\n",
    "        \n",
    "        # Setup NLSA parameters for current psi number.\n",
    "        num = DD.shape[1]  # Total number of images after reordering.\n",
    "        k = num - con_order  # Adjustment for conformation order.\n",
    "        NLSAPar = dict(num=num, ConOrder=con_order, k=k, tune=p.tune, nS=nS, save=False, psiTrunc=psi_trunc)\n",
    "        \n",
    "        # Perform NLSA and gather results.\n",
    "        IMGT, Topo_mean, psirec, psiC1, sdiag, VX, mu, tau = _NLSA(NLSAPar, DD, pos_path, pos_psi1, imgAll, msk2, CTF, extra_params)\n",
    "        \n",
    "        # Post-processing and result saving.\n",
    "        n_s_recon = min(IMGT.shape)  # Determine the size of the reconstructed image set.\n",
    "        numclass = min(p.nClass, n_s_recon // 2)  # Determine the number of classes based on available data.\n",
    "        \n",
    "        # Normalize tau values.\n",
    "        tau = (tau - min(tau)) / (max(tau) - min(tau))\n",
    "        \n",
    "        # Processing for image classification and tau adjustment.\n",
    "        tauinds = []  # Initialize list to hold indices of images within each class.\n",
    "        i1 = 0  # Start index for processing.\n",
    "        i2 = IMGT.shape[0]  # End index for processing.\n",
    "        IMG1 = np.zeros((i2, numclass), dtype='float64')  # Initialize matrix to hold class-wise images.\n",
    "        \n",
    "        # Classify images based on normalized tau values.\n",
    "        for i in range(numclass):\n",
    "            ind1 = float(i) / numclass\n",
    "            ind2 = ind1 + 1. / numclass\n",
    "            if i == numclass - 1:\n",
    "                tauind = np.where((tau >= ind1) & (tau <= ind2))[0]\n",
    "            else:\n",
    "                tauind = np.where((tau >= ind1) & (tau < ind2))[0]\n",
    "            \n",
    "            # Adjust classification boundaries if necessary.\n",
    "            while tauind.size == 0:\n",
    "                scale_correction = 1. / (numclass * 2.)\n",
    "                ind1 -= scale_correction * ind1\n",
    "                ind2 += scale_correction * ind2\n",
    "                tauind = np.where((tau >= ind1) & (tau < ind2))[0]\n",
    "            \n",
    "            # Assign images to classes based on tau indices.\n",
    "            IMG1[i1:i2, i] = IMGT[:, tauind[0]]\n",
    "            tauinds.append(tauind[0])\n",
    "        \n",
    "        # Further processing for extended analysis or result saving.\n",
    "        if is_full:\n",
    "            # Adjust tau by comparing IMG1s for a second pass analysis.\n",
    "            psi2_file = f'{psi2_file}_psi_{psinum}'\n",
    "            data = fin1(psi2_file)\n",
    "            IMG1a = data['IMG1']\n",
    "            dc = _diff_corr(IMG1, IMG1a, numclass - 1)\n",
    "            \n",
    "            # Adjust tau direction based on correlation results.\n",
    "            if (senses[0] == -1 and dc > 0) or (senses[0] == 1 and dc < 0):\n",
    "                tau = 1 - tau\n",
    "            \n",
    "            # Save results for the extended analysis.\n",
    "            out_file = f'{EL_file}_{traj_name}_1'\n",
    "            fout1(out_file, IMG1=IMG1, IMGT=IMGT, posPath=pos_path, PosPsi1=pos_psi1, psirec=psirec, tau=tau, psiC1=psiC1, mu=mu, VX=VX, sdiag=sdiag, Topo_mean=Topo_mean, tauinds=tauinds)\n",
    "        else:\n",
    "            # Save results for the initial analysis pass.\n",
    "            out_file = f'{psi2_file}_psi_{psinum}'\n",
    "            fout1(out_file, IMG1=IMG1, psirec=psirec, tau=tau, psiC1=psiC1, mu=mu, VX=VX, sdiag=sdiag, Topo_mean=Topo_mean, tauinds=tauinds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225f76c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _construct_input_data(N):\n",
    "    \"\"\"\n",
    "    Constructs the input data configuration for N processing jobs.\n",
    "\n",
    "    This function prepares the necessary file paths and parameters for each job,\n",
    "    based on the global configuration object 'p'. It organizes the data needed for\n",
    "    subsequent analysis, including file paths for distance matrices, psi vectors,\n",
    "    secondary psi files, and eigenvalue lists, along with the job-specific psi numbers\n",
    "    and their corresponding senses.\n",
    "\n",
    "    Parameters:\n",
    "    - N (int): The number of processing jobs or datasets to prepare input data for.\n",
    "\n",
    "    Returns:\n",
    "    - list: A list of lists, where each inner list contains the input data for a single job,\n",
    "            including file paths and psi number configurations.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize the list to hold input data configurations for each job.\n",
    "    ll = []\n",
    "    \n",
    "    # Generate arrays for psi numbers and senses for all jobs, ensuring each job has a complete set.\n",
    "    psi_nums_all = np.tile(np.array(range(p.num_psis)), (N, 1))  # Duplicate psi number range for each job.\n",
    "    senses_all = np.tile(np.ones(p.num_psis), (N, 1))  # Assume a default sense (directionality) of +1 for all psi numbers.\n",
    "\n",
    "    # Iterate over each job to construct its specific input data configuration.\n",
    "    for prD in range(N):\n",
    "        # Retrieve file paths for the current job's data, utilizing methods from the global 'p' object.\n",
    "        dist_file = p.get_dist_file(prD)  # Distance matrix file path.\n",
    "        psi_file = p.get_psi_file(prD)  # Psi vectors file path.\n",
    "        psi2_file = p.get_psi2_file(prD)  # Secondary psi vectors file path (for additional analysis or comparison).\n",
    "        EL_file = p.get_EL_file(prD)  # Eigenvalues list file path.\n",
    "\n",
    "        # Extract the psi numbers and their senses for the current job.\n",
    "        psinums = psi_nums_all[prD, :]  # Psi numbers used in this job.\n",
    "        senses = senses_all[prD, :]  # Corresponding senses for the psi numbers.\n",
    "\n",
    "        # Prepare a list of psi indices for incomplete datasets or specific analysis requirements.\n",
    "        psi_list = list(range(len(psinums)))  # This could be customized based on data completeness or analysis focus.\n",
    "\n",
    "        # Append the constructed configuration for the current job to the main list.\n",
    "        ll.append([dist_file, psi_file, psi2_file, EL_file, psinums, senses, prD, psi_list])\n",
    "\n",
    "    # Return the compiled list of input data configurations for all jobs.\n",
    "    return ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd18c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def op(*argv):\n",
    "    \"\"\"\n",
    "    Orchestrates the Nonlinear Local Spectral Analysis (NLSA) snapshots computation for a given dataset.\n",
    "    This function manages the loading of input data, setting up multiprocessing for concurrent analysis,\n",
    "    and tracking progress of the computations. It supports both GUI and command-line progress reporting.\n",
    "\n",
    "    Parameters:\n",
    "        *argv: Variable length argument list. The presence of any argument enables GUI progress reporting.\n",
    "               The first argument, if present, is expected to be an instance capable of emitting progress\n",
    "               updates to a GUI.\n",
    "\n",
    "    Note:\n",
    "        - The function initializes multiprocessing with the 'fork' start method to ensure compatibility\n",
    "          and performance across different operating systems.\n",
    "        - It conditionally handles progress reporting based on the execution context (GUI or command-line)\n",
    "          and the number of CPU cores utilized for the analysis.\n",
    "        - The actual analysis is performed by a partial function 'local_psi_func', which is a wrapper around\n",
    "          'psi_analysis_single', configured with parameters from a global parameter object 'p'.\n",
    "        - The computation is distributed across multiple processes if more than one CPU core is available,\n",
    "          otherwise, it falls back to sequential processing.\n",
    "\n",
    "    Raises:\n",
    "        Exception: If there are issues initializing the multiprocessing environment or during the execution\n",
    "                   of the analysis functions.\n",
    "\n",
    "    Example usage:\n",
    "        op() # Command-line mode, no progress updates to GUI.\n",
    "        op(progress_emitter) # GUI mode, with 'progress_emitter' handling progress updates.\n",
    "    \"\"\"\n",
    "    print(\"Computing the NLSA snapshots...\")\n",
    "    p.load()\n",
    "    multiprocessing.set_start_method('fork', force=True)\n",
    "    use_gui_progress = len(argv) > 0\n",
    "    input_data = _construct_input_data(p.numberofJobs)\n",
    "    n_jobs = len(input_data)\n",
    "    progress3 = argv[0] if use_gui_progress else NullEmitter()\n",
    "    local_psi_func = partial(psi_analysis_single,\n",
    "                             con_order_range=p.conOrderRange,\n",
    "                             traj_name=p.trajName,\n",
    "                             is_full=0,\n",
    "                             psi_trunc=p.num_psiTrunc)\n",
    "\n",
    "    if p.ncpu == 1:\n",
    "        for i, datai in tqdm.tqdm(enumerate(input_data), total=n_jobs, disable=use_gui_progress):\n",
    "            local_psi_func(datai)\n",
    "            progress3.emit(int(99 * i / n_jobs))\n",
    "    else:\n",
    "        with multiprocessing.Pool(processes=p.ncpu) as pool:\n",
    "            for i, _ in tqdm.tqdm(enumerate(pool.imap_unordered(local_psi_func, input_data)),\n",
    "                                  total=n_jobs,\n",
    "                                  disable=use_gui_progress):\n",
    "                progress3.emit(int(99 * i / n_jobs))\n",
    "\n",
    "    p.save()\n",
    "    progress3.emit(100)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1fa2397f",
   "metadata": {},
   "source": [
    "op()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8679bde",
   "metadata": {},
   "source": [
    "### Step 1: Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0114ded8",
   "metadata": {},
   "outputs": [],
   "source": [
    "p.load()\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf51abf",
   "metadata": {},
   "source": [
    "### Step 2: Construct Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2e83bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding and trimming manifold from particles\n",
    "input_data = _construct_input_data(p.numberofJobs)\n",
    "input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613a8b8a",
   "metadata": {},
   "source": [
    "### Step 3: Define the Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58cb7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_psi_func = partial(psi_analysis_single,\n",
    "                         con_order_range=p.conOrderRange,\n",
    "                         traj_name=p.trajName,\n",
    "                         is_full=0,\n",
    "                         psi_trunc=p.num_psiTrunc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6174d8e2",
   "metadata": {},
   "source": [
    "### Step 4: Process Datasets Sequentially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9abcdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_jobs = len(input_data)\n",
    "for i, datai in tqdm.tqdm(enumerate(input_data), total=n_jobs):\n",
    "    local_psi_func(datai)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9e9717",
   "metadata": {},
   "source": [
    "### Step 5: Save Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36b8224",
   "metadata": {},
   "outputs": [],
   "source": [
    "p.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56864825",
   "metadata": {},
   "source": [
    "## Illustration of the steps involved"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982d3ff3",
   "metadata": {},
   "source": [
    "### Step 1: Select a Projection Direction for Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ee346a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take one of the prDs for analyis among all the prDs by input_data[i]\n",
    "# In this case, we select the projection direction 2\n",
    "n=2\n",
    "dist_file_sample = input_data[n][0]  # File containing the distance matrix\n",
    "print(dist_file_sample)\n",
    "psi_file_sample = input_data[n][1]  # File containing diffusion map\n",
    "print(psi_file_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9f5b23",
   "metadata": {},
   "source": [
    "### Step 2: Load Data from Specified Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf096f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_IMG = fin1(dist_file_sample)  # Load image data, including distance matrix and image matrix\n",
    "print(\"Keys for the image data:\", data_IMG.keys())\n",
    "data_psi = fin1(psi_file_sample)  # Load psi data, including psi coordinates and positional paths\n",
    "print(\"Keys for the psi data:\", data_psi.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aedffea1",
   "metadata": {},
   "source": [
    "### Step 3: Preprocess and Setup Before Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc428f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3A: Load the parameters\n",
    "prD = input_data[n][6]  # Processing job identifier.\n",
    "con_order_range=p.conOrderRange\n",
    "print(\"con_order_range:\", con_order_range)\n",
    "extra_params = dict(outDir='', prD=prD)\n",
    "psi_trunc=p.num_psiTrunc\n",
    "print(\"psi_trunc:\", psi_trunc)\n",
    "psinums = input_data[n][4]  # Array of psi numbers to be analyzed\n",
    "print(\"psinums:\", psinums)\n",
    "psi_list = input_data[n][7] if len(input_data) == 8 else psinums\n",
    "print(\"psi_list:\", psi_list)\n",
    "\n",
    "# Step 3B: Preprocess \n",
    "D = np.array(data_IMG['D'])  # Convert distance matrix to numpy array\n",
    "print(\"The shape of the distance matrix is:\", D.shape)\n",
    "imgAll = np.array(data_IMG['imgAll'])  # Convert images matrix to numpy array\n",
    "print(\"The shape of the imgAll is:\", imgAll.shape)\n",
    "msk2 = np.array(data_IMG['msk2'])  # Volume mask to be used after CTF application\n",
    "CTF = np.array(data_IMG['CTF'])  # CTF values for each image\n",
    "psi = data_psi['psi']  # Psi coordinates for images\n",
    "print(\"The shape of the psi is:\", psi.shape)\n",
    "pos_path = data_psi['posPath']  # Indices of images in the path\n",
    "nS = len(pos_path)  # Total number of images\n",
    "print(\"Total number of images:\", nS)\n",
    "con_order = nS // con_order_range  # Determination of conformation order\n",
    "print(\"The conformational order is:\", con_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c47313d",
   "metadata": {},
   "source": [
    "### Step 4: Reshape and Reorganize Data for Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b76050",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = int(np.sqrt(imgAll.size / D.shape[0]))  # Calculate dimension size\n",
    "print(\"Number of dimensions:\", dim)\n",
    "CTF = CTF.reshape(D.shape[0], dim, dim)  # Reshape CTF values\n",
    "pos_path = np.squeeze(pos_path)  # Remove single-dimensional entries from pos_path\n",
    "D = D[pos_path][:, pos_path]  # Rearrange distance matrix according to pos_path\n",
    "print(\"The shape of rearranged distance matrix is:\", D.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a2fc95",
   "metadata": {},
   "source": [
    "### Step 5: Organize Data for Manifold Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89d117a",
   "metadata": {},
   "outputs": [],
   "source": [
    "psinum = 0  # Index for the psi direction of interest\n",
    "psi_sorted_indices = np.argsort(psi[:, psinum])  # Sort indices based on psi values\n",
    "pos_psi1 = psi_sorted_indices  # Sorted indices for reordering images\n",
    "DD = D[pos_psi1][:, pos_psi1]  # Reorganized distance matrix\n",
    "print(\"The shape of reorganized distance matrix is:\", DD.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28ceda6",
   "metadata": {},
   "source": [
    "### Step 6: Perform NLSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c005e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for psinum in psi_list:\n",
    "    if psinum == -1:\n",
    "        continue  # Skip processing if psi number is marked as -1\n",
    "    # Sort indices based on current psi number\n",
    "    psi_sorted_ind = np.argsort(psi[:, psinum])\n",
    "    pos_psi1 = psi_sorted_ind  # Use sorted indices for reordering images\n",
    "    # Reorganize distance matrix based on sorted psi indices\n",
    "    DD = D[pos_psi1][:, pos_psi1]\n",
    "    # Setup NLSA parameters for current psi number\n",
    "    num = DD.shape[1]  # Total number of images after reordering\n",
    "    k = num - con_order  # Adjustment for conformation order\n",
    "    NLSAPar = dict(num=num, ConOrder=con_order, k=k, tune=p.tune, nS=nS, save=False, psiTrunc=psi_trunc)\n",
    "    # Perform NLSA and gather results\n",
    "    IMGT, Topo_mean, psirec, psiC1, sdiag, VX, mu, tau = _NLSA(NLSAPar, DD, pos_path, pos_psi1, imgAll, msk2, CTF, extra_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc52f64d",
   "metadata": {},
   "source": [
    "### Step 7: Visualize Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f2124c",
   "metadata": {},
   "source": [
    "#### 1. Visualizing Reconstructed Images (IMGT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bda7381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where 'dim' is the dimension of the images and let us an image index to visualize\n",
    "image_indices = [0, 25, 50, 75, 100, 150, 200, 250, 275]  # indices\n",
    "# Set up the subplot grid\n",
    "fig, axes = plt.subplots(3, 3, figsize=(12, 12))\n",
    "# Flatten axes for easy iteration\n",
    "axes_flat = axes.flatten()\n",
    "# Loop through the selected image indices\n",
    "for i, image_index in enumerate(image_indices):\n",
    "    # Reshape the column from IMGT to its original 2D image shape\n",
    "    image = IMGT[:, image_index].reshape(dim, dim)\n",
    "    # Visualize the image\n",
    "    ax = axes_flat[i]\n",
    "    im = ax.imshow(image, cmap='gray')\n",
    "    ax.set_title(f'Image {image_index}')\n",
    "    ax.axis('off')  # Hide axes ticks\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "# Add a colorbar\n",
    "fig.colorbar(im, ax=axes.ravel().tolist(), shrink=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc6dee3",
   "metadata": {},
   "source": [
    "#### 2. Analyzing Mean Topological Features (Topo_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311df96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the first topological features as an image\n",
    "# Set up the subplot grid\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 10))  # Adjust figsize as needed\n",
    "# Loop through all 8 topological features\n",
    "for i in range(8):\n",
    "    # Select the current topological feature\n",
    "    topo_feature = Topo_mean[:, i].reshape(dim, dim)\n",
    "    # Determine the current subplot\n",
    "    ax = axes[i // 4, i % 4]  # Integer division and modulo for row and column indices\n",
    "    # Visualize the topological feature\n",
    "    im = ax.imshow(topo_feature, cmap='gray')\n",
    "    ax.set_title(f'Topological Feature {i+1}')\n",
    "    ax.axis('off')  # Hide axes ticks\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "# Add a colorbar\n",
    "fig.colorbar(im, ax=axes.ravel().tolist(), shrink=0.5, aspect=40)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cb5015",
   "metadata": {},
   "source": [
    "#### 3. Distribution of Temporal Parameters (tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c82a764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of Temporal Parameters (tau)\n",
    "plt.hist(tau, bins=50, color='blue', alpha=0.7)\n",
    "plt.title('Distribution of Tau Values')\n",
    "plt.xlabel('Tau')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3194c13e",
   "metadata": {},
   "source": [
    "#### 4. Eigenvector Visualization (psirec and psiC1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6451676f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the First 8 Eigenvectors in a 2x4 Grid\n",
    "num_eigenvectors_psirec = psirec.shape[1]\n",
    "num_eigenvectors_psiC1 = psiC1.shape[1]\n",
    "print(f\"Number of reconstructed eigenvectors (psirec): {num_eigenvectors_psirec}\")\n",
    "print(f\"Number of original eigenvectors (psiC1): {num_eigenvectors_psiC1}\")\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 10))  # Adjust figsize as needed\n",
    "for i in range(8):  # Plotting the first 8 eigenvectors\n",
    "    ax = axes[i // 4, i % 4]  # Determine the current subplot for a 2x4 grid\n",
    "    ax.plot(psirec[:, i], label=f'Reconstructed Psi {i+1}')\n",
    "    ax.plot(psiC1[:, i], label=f'Original Psi {i+1}', linestyle='--')\n",
    "    ax.set_title(f'Eigenvector {i+1}')\n",
    "    ax.set_xlabel('Component Index')\n",
    "    ax.set_ylabel('Value')\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcc2200",
   "metadata": {},
   "source": [
    "#### 5. Singular Values (sdiag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf54130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Singular Values (sdiag)\n",
    "plt.plot(sdiag, marker='o', linestyle='-', color='r')\n",
    "plt.title('Singular Values')\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Singular Value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e7cc01",
   "metadata": {},
   "source": [
    "## NLSA example\n",
    "\n",
    "Let us consider a small, synthetic dataset for demonstration purposes. Imagine we have a dataset composed of points that lie on a nonlinear curve or manifold within a higher-dimensional space. Our goal is to use  Nonlinear Laplacian Spectral Analysis (NLSA) to uncover the underlying structure of this data. \n",
    "\n",
    "**Step 1: Constructing the Graph** \n",
    "\n",
    "**Data Representation** \n",
    "\n",
    "Suppose our dataset consists of points X = {x_1, x_2, ..., x_n} in a high-dimensional space.\n",
    "\n",
    "**Graph Construction** \n",
    "\n",
    "We construct a graph where each point x_i is a node. Nodes are connected to their k nearest neighbors based on some distance metric (e.g., Euclidean distance).\n",
    "\n",
    "**Step 2: Creating the Laplacian Matrix** \n",
    "\n",
    "**Weight Matrix** \n",
    "\n",
    "Create a weight matrix W, where W_ij represents the weight (or similarity) between points x_i and x_j. This could be an exponential function of their distance, for instance, W_ij = exp(-||x_i - x_j||^2 / t), where t is a scaling parameter. \n",
    "\n",
    "**Degree Matrix** \n",
    "\n",
    "Construct a diagonal degree matrix D, where each diagonal element D_ii is the sum of the weights of all edges connected to node i.\n",
    "\n",
    "**Laplacian Matrix** \n",
    "\n",
    "The Laplacian matrix L is defined as L = D - W. \n",
    "\n",
    "**Step 3: Eigenvalue Decomposition** \n",
    "\n",
    "**Computing Eigenvalues and Eigenvectors** \n",
    "\n",
    "Perform eigenvalue decomposition on the Laplacian matrix L. This gives us a set of eigenvalues and their corresponding eigenvectors.\n",
    "\n",
    "**Low-Dimensional Representation** \n",
    "\n",
    "Select the top k eigenvectors corresponding to the smallest non-zero eigenvalues. These eigenvectors provide a low-dimensional representation of our data. \n",
    "\n",
    "**Step 4: Analysis and Interpretation** \n",
    "\n",
    "**Interpreting the Eigenvectors** \n",
    "\n",
    "Each of the selected eigenvectors represents a dimension in the lower-dimensional space. Points that were close on the manifold in the high-dimensional space will be close in this new space.\n",
    "\n",
    "**Visualization** \n",
    "\n",
    "If we choose two or three eigenvectors, we can visualize the data in 2D or 3D, revealing its intrinsic structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eaf3d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a synthetic 'Swiss Roll' dataset for demonstration\n",
    "n_samples = 800\n",
    "noise = 0.05\n",
    "X, _ = make_swiss_roll(n_samples, noise=noise)\n",
    "X = X[:, [0, 2]]  # Simplifying to 2D for easier visualization\n",
    "\n",
    "# Construct the k-nearest neighbors graph\n",
    "k = 10  # number of neighbors\n",
    "A = kneighbors_graph(X, k, mode='connectivity', include_self=True)\n",
    "\n",
    "# Create the Laplacian matrix\n",
    "L = laplacian(A, normed=True)\n",
    "\n",
    "# Compute the eigenvalues and eigenvectors for sparse matrix\n",
    "# Since we are only interested in the smallest non-zero eigenvalues, 'which' parameter is set to 'SM'\n",
    "# (which stands for smallest magnitude)\n",
    "eigenvalues, eigenvectors = eigsh(L, k=k+1, which='SM')\n",
    "\n",
    "# Low-dimensional representation\n",
    "# We skip the first eigenvector (corresponding to the smallest eigenvalue which is zero)\n",
    "low_dim_data = eigenvectors[:, 1:3]\n",
    "\n",
    "# Visualizing the original and the transformed data\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Original Data\n",
    "ax[0].scatter(X[:, 0], X[:, 1], c='blue', s=10)\n",
    "ax[0].set_title('Original Data (Swiss Roll)')\n",
    "\n",
    "# Transformed Data\n",
    "ax[1].scatter(low_dim_data[:, 0], low_dim_data[:, 1], c='red', s=10)\n",
    "ax[1].set_title('Low-Dimensional Representation')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef76c50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
